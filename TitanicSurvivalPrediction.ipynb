{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TitanicSurvivalPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Note :** This whole model building process is through sklearn pipeline.This is a very good beginning if you would like to know how in real world models are build.Refer sklearn documentation while working with ColumnTransformer and Sklearn Pipeline if you are not familiar with it already.<br>\n",
        "<br>\n",
        "To keep this notebook clear I have not included any Exploratory data analysis part or any fancy plot. You can do plotting and mess with the data as per your wish.<br>\n",
        "\n",
        "The preprocessed data is pickled and povided here."
      ],
      "metadata": {
        "id": "N3V9pwq_kDYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "AL9A1O3ThLyy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "8y7an5uw5JlO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline,make_pipeline\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports for model"
      ],
      "metadata": {
        "id": "TgG1qm3NhInK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "DT = DecisionTreeClassifier()\n",
        "RF = RandomForestClassifier()\n",
        "LR = LogisticRegression()"
      ],
      "metadata": {
        "id": "QjxseYi0r8Pq"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports for feature scaling"
      ],
      "metadata": {
        "id": "Xpk9C0vAhWrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "minmaxscaler = MinMaxScaler()"
      ],
      "metadata": {
        "id": "pJXq-9ObQoNa"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports for feature selection"
      ],
      "metadata": {
        "id": "XzAwnDq8hbEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest,chi2"
      ],
      "metadata": {
        "id": "uVBpjJjRbLyA"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display Pipeline\n",
        "from sklearn import set_config\n",
        "set_config(display='diagram')"
      ],
      "metadata": {
        "id": "k_qCJHHDFA3L"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*IMPORT DATA*"
      ],
      "metadata": {
        "id": "Lf7gcenVpWyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pickle.load(open(\"finaltrain.pkl\",\"rb\"))\n",
        "test = pickle.load(open(\"finaltest.pkl\",\"rb\"))\n",
        "passengerId = pickle.load(open(\"passengerId.pkl\",\"rb\"))"
      ],
      "metadata": {
        "id": "mh-DDtEF-QkH"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Handling Outliers*"
      ],
      "metadata": {
        "id": "tcmCnec6C7N6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Age Column"
      ],
      "metadata": {
        "id": "Fnh8K77LJP_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion after visualisation<br>\n",
        "Age column - little skewed and normal distributed<br>\n",
        "Fare column - highly skewed distribution<br>\n",
        "Both of the column contains outlier\n"
      ],
      "metadata": {
        "id": "je4hOeR9hld9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Outlier handling using IQR technique*"
      ],
      "metadata": {
        "id": "Adzx2YS5iK2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the IQR\n",
        "percentile25 = train['Age'].quantile(0.25)\n",
        "percentile75 = train['Age'].quantile(0.75)\n",
        "iqr = percentile75 - percentile25\n",
        "#Limit for age\n",
        "High_for_age = percentile75 + 1.5 * iqr\n",
        "Low_for_age = percentile25 - 1.5 * iqr"
      ],
      "metadata": {
        "id": "90UaaJV7DSDw"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For train data\n",
        "train1 = train.copy()\n",
        "\n",
        "train1['Age'] = np.where(\n",
        "    train1['Age'] > High_for_age,\n",
        "    High_for_age,\n",
        "    np.where(\n",
        "        train1['Age'] < Low_for_age,\n",
        "        Low_for_age,\n",
        "        train1['Age']\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "tzhUfE3aPvDZ"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For test data\n",
        "test1 = test.copy()\n",
        "\n",
        "test1['Age'] = np.where(\n",
        "    test1['Age'] > High_for_age,\n",
        "    High_for_age,\n",
        "    np.where(\n",
        "        test1['Age'] < Low_for_age,\n",
        "        Low_for_age,\n",
        "        test1['Age']\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "ChudXmRbQd69"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fare Column"
      ],
      "metadata": {
        "id": "A7cHnqx0JUkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the IQR\n",
        "percentile25 = train['Fare'].quantile(0.25)\n",
        "percentile75 = train['Fare'].quantile(0.75)\n",
        "iqr = percentile75 - percentile25\n",
        "#Limit for fare\n",
        "upper_limit = percentile75 + 1.5 * iqr\n",
        "lower_limit = percentile25 - 1.5 * iqr"
      ],
      "metadata": {
        "id": "fnNedGo7_Vcx"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train2 = train1.copy()\n",
        "\n",
        "train2['Fare'] = np.where(\n",
        "    train2['Fare'] > upper_limit,\n",
        "    upper_limit,\n",
        "    np.where(\n",
        "        train2['Fare'] < lower_limit,\n",
        "        lower_limit,\n",
        "        train2['Fare']\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "vYRYDN2yL7Xu"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2 = test1.copy()\n",
        "\n",
        "test2['Fare'] = np.where(\n",
        "    test2['Fare'] > upper_limit,\n",
        "    upper_limit,\n",
        "    np.where(\n",
        "        test2['Fare'] < lower_limit,\n",
        "        lower_limit,\n",
        "        test2['Fare']\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "pLqLSBlARK8Y"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing train test split"
      ],
      "metadata": {
        "id": "uVl6GRfaiT2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(train2.drop(columns=[\"Survived\"]),train2[\"Survived\"],test_size=0.2,random_state=20)"
      ],
      "metadata": {
        "id": "8nDi4GQ2-fBe"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating transformer using ColumnTransformer for differentcoperation**"
      ],
      "metadata": {
        "id": "UsWrVUTS3SZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*ColumnTransformer for IMPUTATION*"
      ],
      "metadata": {
        "id": "l-u1-Cq_3SSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer1=ColumnTransformer([\n",
        "                          (\"Mean Imputation\",SimpleImputer(),[2]),\n",
        "                          (\"Most frequent imputation\",SimpleImputer(strategy=\"most_frequent\"),[4])\n",
        "],remainder=\"passthrough\") "
      ],
      "metadata": {
        "id": "yx6TvXHj63H1"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*ColumnTransformer for FEATURE ENCODING*"
      ],
      "metadata": {
        "id": "9YbXBjdxpAif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer for encoding\n",
        "transformer2 = ColumnTransformer([\n",
        "                                  (\"OHE\",OneHotEncoder(sparse=False),[1,3,6])                               \n",
        "],remainder=\"passthrough\")"
      ],
      "metadata": {
        "id": "99pUu5i18gaR"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Creating an instance FEATURE SCALING*"
      ],
      "metadata": {
        "id": "yI121X9FR4Kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating instance for different Scaling techniques\n",
        "minmaxscaler = MinMaxScaler()"
      ],
      "metadata": {
        "id": "9cI5w2AjR9XR"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL FITTING WITH PIPE CREATION"
      ],
      "metadata": {
        "id": "5tNl7oZUpJC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*PIPELINE FOR DECISION TREE*"
      ],
      "metadata": {
        "id": "n9PK2qY7jpQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipe1 = Pipeline([\n",
        "                 ('imputation',transformer1),\n",
        "                 (\"encoding\",transformer2),\n",
        "                 (\"StandardSCaler\",minmaxscaler),\n",
        "                 ('Feature selection',SelectKBest(k=3)),\n",
        "                 (\"model fitting\",DT)\n",
        "])\n",
        "pipe1.fit(X_train,y_train)\n",
        "y_pred1 = pipe1.predict(X_test)\n",
        "accuracy_score(y_test,y_pred1)"
      ],
      "metadata": {
        "id": "9_oRW78k9vSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c49b6e-e375-46db-cafe-fdc321f84986"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8324022346368715"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To display pipeline visually\n",
        "pipe1.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "8Ojti8mbjl3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*PIPELINE FOR RANDOM FOREST*"
      ],
      "metadata": {
        "id": "wZ-ZIvCwjwkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PIPELINE FOR RANDOM FOREST\n",
        "pipe2 = Pipeline([\n",
        "                 ('imputation',transformer1),\n",
        "                 (\"encoding\",transformer2),\n",
        "                 (\"Scaling\",minmaxscaler),\n",
        "                 ('Feature selection',SelectKBest(chi2,k=3)),\n",
        "                 (\"model fitting\",RF)\n",
        "])\n",
        "pipe2.fit(X_train,y_train)\n",
        "y_pred2 = pipe2.predict(X_test)\n",
        "accuracy_score(y_test,y_pred2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQJ1Zm3mmc1U",
        "outputId": "f3a57ec2-74ce-49b4-bbaf-a27b407bb078"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8324022346368715"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To display pipeline visually\n",
        "pipe2.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "BlHaOticjTg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*PIPELINE FOR LOGISTIC REGRESSION*"
      ],
      "metadata": {
        "id": "TWCs4gxWj7CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PIPELINE FOR LOGISTIC REGRESSION\n",
        "pipe3 = Pipeline([\n",
        "                 ('imputation',transformer1),\n",
        "                 (\"encoding\",transformer2),\n",
        "                 (\"Scaling\",minmaxscaler),\n",
        "              #   ('Feature selection',SelectKBest(chi2,k=10)),\n",
        "                 (\"model fitting\",LR)\n",
        "])\n",
        "pipe3.fit(X_train,y_train)\n",
        "y_pred3 = pipe3.predict(X_test)\n",
        "accuracy_score(y_test,y_pred3)"
      ],
      "metadata": {
        "id": "UqHFmta9nw2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787b0578-7989-4381-80d1-e992a967a879"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8715083798882681"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To display pipeline visually\n",
        "pipe3.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "1MI62Pd2jFyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sumission dataframe creation**"
      ],
      "metadata": {
        "id": "aTSiQY4kN_el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Model"
      ],
      "metadata": {
        "id": "Yj_y1LCSVcdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe1.fit(train2.drop(columns=[\"Survived\"]),train2[\"Survived\"])\n",
        "final_predict = pipe1.predict(test2)\n",
        "subdata = pd.DataFrame()\n",
        "subdata[\"PassengerId\"] = passengerId\n",
        "subdata[\"Survived\"] = final_predict\n",
        "subdata.to_csv(\"submission_DT.csv\",index=False)"
      ],
      "metadata": {
        "id": "ooiEwPMLLcF_"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Model"
      ],
      "metadata": {
        "id": "YXU6iqNfVz10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe2.fit(train2.drop(columns=[\"Survived\"]),train2[\"Survived\"])\n",
        "final_predict = pipe2.predict(test2)\n",
        "subdata = pd.DataFrame()\n",
        "subdata[\"PassengerId\"] = passengerId\n",
        "subdata[\"Survived\"] = final_predict\n",
        "subdata.to_csv(\"submission_RF.csv\",index=False)"
      ],
      "metadata": {
        "id": "FLoBTufsVhbO"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression Model"
      ],
      "metadata": {
        "id": "EsrsjZHKV3lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe3.fit(train2.drop(columns=[\"Survived\"]),train2[\"Survived\"])\n",
        "final_predict = pipe2.predict(test2)\n",
        "subdata = pd.DataFrame()\n",
        "subdata[\"PassengerId\"] = passengerId\n",
        "subdata[\"Survived\"] = final_predict\n",
        "subdata.to_csv(\"submission_LR.csv\",index=False)"
      ],
      "metadata": {
        "id": "o31YuQrdVvEU"
      },
      "execution_count": 184,
      "outputs": []
    }
  ]
}